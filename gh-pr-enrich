#!/bin/bash
# gh-pr-enrich - GitHub CLI extension for comprehensive PR analysis
# Fetches PR details, comments, and optionally enriches with Claude AI analysis.
#
# Installation:
#   gh extension install bl4ck3lk/gh-pr-enrich
#
# Usage:
#   gh pr-enrich <PR_NUMBER> [--json] [--markdown] [--output-dir <DIR>] [--enrich] [--diff]
#   gh pr-enrich resolve <THREAD_ID> [THREAD_ID...]
#   gh pr-enrich watch <PR_NUMBER> [--interval MIN] [--enrich] [--notify]
#   gh pr-enrich address <PR_NUMBER>
#   gh pr-enrich install-skill | uninstall-skill
#
# Examples:
#   gh pr-enrich 123                    # Fetch PR details
#   gh pr-enrich 123 --enrich           # Fetch + Claude analysis
#   gh pr-enrich 123 --enrich --diff    # Analysis with code context
#   gh pr-enrich 123 --json             # Output JSON only
#   gh pr-enrich resolve PRRT_xxx       # Resolve a comment thread
#   gh pr-enrich watch 123 --enrich     # Monitor PR for changes
#   gh pr-enrich address 123            # Interactive issue fixing

set -e

VERSION="1.0.0"

# Handle --version flag
if [ "$1" = "--version" ] || [ "$1" = "-v" ]; then
    echo "gh-pr-enrich version $VERSION"
    exit 0
fi

# Cross-platform path canonicalization (works on both BSD/macOS and GNU/Linux)
# Returns absolute path, resolving symlinks
canonicalize_path() {
    local path="$1"
    if [ -d "$path" ]; then
        (cd "$path" && pwd -P)
    elif [ -L "$path" ]; then
        local target
        target=$(readlink "$path")
        if [[ "$target" = /* ]]; then
            # Absolute symlink target
            canonicalize_path "$target"
        else
            # Relative symlink target - resolve from symlink's directory
            local dir
            dir=$(dirname "$path")
            canonicalize_path "$dir/$target"
        fi
    elif [ -f "$path" ]; then
        local dir
        dir=$(cd "$(dirname "$path")" && pwd -P)
        echo "$dir/$(basename "$path")"
    else
        # Path doesn't exist, just normalize it
        echo "$path"
    fi
}

# Handle install-skill subcommand
if [ "$1" = "install-skill" ]; then
    SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd -P)"
    SKILL_SOURCE="$SCRIPT_DIR/.claude/skills/gh-pr-enrich"
    SKILL_TARGET="$HOME/.claude/skills/gh-pr-enrich"
    SKILLS_DIR="$HOME/.claude/skills"

    # Check if skill exists in extension
    if [ ! -d "$SKILL_SOURCE" ]; then
        echo "Error: Skill not found at $SKILL_SOURCE"
        echo "Try upgrading the extension: gh extension upgrade pr-enrich"
        exit 1
    fi

    # Pre-flight validation: Check ~/.claude/skills path
    if [ -f "$SKILLS_DIR" ]; then
        echo "Error: $SKILLS_DIR exists as a regular file, not a directory."
        echo "Remove it first: rm $SKILLS_DIR"
        exit 1
    fi

    # Create skills directory if needed
    if [ ! -d "$SKILLS_DIR" ]; then
        if ! mkdir -p "$SKILLS_DIR" 2>/dev/null; then
            echo "Error: Could not create directory $SKILLS_DIR"
            echo "Check permissions on $HOME/.claude"
            exit 1
        fi
    fi

    # Pre-flight validation: Check target path
    if [ -f "$SKILL_TARGET" ] && [ ! -L "$SKILL_TARGET" ]; then
        echo "Error: $SKILL_TARGET exists as a regular file, not a symlink."
        echo "Remove it first: rm $SKILL_TARGET"
        exit 1
    fi

    # Check if already installed (with proper path canonicalization)
    if [ -L "$SKILL_TARGET" ]; then
        CURRENT_CANONICAL=$(canonicalize_path "$SKILL_TARGET")
        SOURCE_CANONICAL=$(canonicalize_path "$SKILL_SOURCE")
        if [ "$CURRENT_CANONICAL" = "$SOURCE_CANONICAL" ]; then
            echo "‚úÖ Skill already installed at $SKILL_TARGET"
            exit 0
        else
            echo "‚ö†Ô∏è  Existing symlink points to: $(readlink "$SKILL_TARGET")"
            echo "   Updating to: $SKILL_SOURCE"
            rm "$SKILL_TARGET"
        fi
    elif [ -d "$SKILL_TARGET" ]; then
        echo "Error: Directory already exists at $SKILL_TARGET"
        echo "Remove it first if you want to install the symlinked skill:"
        echo "  rm -rf $SKILL_TARGET"
        exit 1
    fi

    # Create symlink
    if ! ln -s "$SKILL_SOURCE" "$SKILL_TARGET" 2>/dev/null; then
        echo "Error: Could not create symlink at $SKILL_TARGET"
        echo "Check permissions on $SKILLS_DIR"
        exit 1
    fi
    echo "‚úÖ Claude skill installed!"
    echo "   Source: $SKILL_SOURCE"
    echo "   Target: $SKILL_TARGET"
    echo ""
    echo "The skill will auto-update when you run: gh extension upgrade pr-enrich"
    exit 0
fi

# Handle retrospective subcommand - analyze patterns across all PR reports
if [ "$1" = "retrospective" ]; then
    shift

    # Default values
    RETRO_SINCE=""
    RETRO_AUTHOR=""
    RETRO_REPORTS_DIR="${PR_REVIEW_OUTPUT_ROOT:-.reports/pr-reviews}"
    RETRO_OUTPUT_DIR=".reports/retrospectives"
    RETRO_ENRICH=false
    RETRO_MIN_PRS=3
    RETRO_FORMAT=""
    RETRO_OUTPUT_FORMAT="combined"

    # Show help
    if [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        cat << 'RETRO_HELP'
Usage: gh pr-enrich retrospective [OPTIONS]

Analyze patterns across all PRs in the reports folder to identify systemic
issues, generate guiding questions, and help prevent recurring problems.

OPTIONS:
  --since DATE        Analyze PRs from this date (ISO 8601 or "30d", "2w")
  --author LOGIN      Filter by PR author(s), comma-separated
  --reports-dir DIR   Path to reports directory (default: .reports/pr-reviews)
  --output-dir DIR    Where to save output (default: .reports/retrospectives)
  --enrich            Use Claude for meta-analysis
  --min-prs N         Warn if fewer than N PRs found (default: 3)
  --format TYPE       Extra output format: claude-md, pr-template, checklist
  --json              Output JSON only
  --markdown          Output Markdown only
  -h, --help          Show this help

EXAMPLES:
  gh pr-enrich retrospective                    # Analyze all PRs
  gh pr-enrich retrospective --since 30d        # Last 30 days
  gh pr-enrich retrospective --author octocat   # Single author
  gh pr-enrich retrospective --enrich           # With Claude meta-analysis
  gh pr-enrich retrospective --format claude-md # Output CLAUDE.md section

OUTPUT:
  The retrospective generates:
  - Cross-PR systemic patterns (issues that repeat across PRs)
  - File/component hotspots (areas receiving most feedback)
  - Guiding questions checklist (derived from recurring issues)
  - Improvement tracking (suggestions made vs adopted)
  - Team retrospective summary (for sprint retros)

  With --enrich, Claude provides:
  - Meta-patterns and root causes
  - Knowledge gap identification
  - Automation opportunities
  - Refined guiding questions
RETRO_HELP
        exit 0
    fi

    # Parse retrospective options
    while [[ $# -gt 0 ]]; do
        case $1 in
            --since)
                if [ -z "${2:-}" ]; then
                    echo "Error: --since requires a date value"
                    exit 1
                fi
                RETRO_SINCE="$2"
                shift 2
                ;;
            --author)
                if [ -z "${2:-}" ]; then
                    echo "Error: --author requires a login value"
                    exit 1
                fi
                RETRO_AUTHOR="$2"
                shift 2
                ;;
            --reports-dir)
                if [ -z "${2:-}" ]; then
                    echo "Error: --reports-dir requires a directory path"
                    exit 1
                fi
                RETRO_REPORTS_DIR="$2"
                shift 2
                ;;
            --output-dir)
                if [ -z "${2:-}" ]; then
                    echo "Error: --output-dir requires a directory path"
                    exit 1
                fi
                RETRO_OUTPUT_DIR="$2"
                shift 2
                ;;
            --enrich)
                RETRO_ENRICH=true
                shift
                ;;
            --min-prs)
                if [ -z "${2:-}" ]; then
                    echo "Error: --min-prs requires a numeric value"
                    exit 1
                fi
                if ! [[ "$2" =~ ^[0-9]+$ ]] || [ "$2" -lt 1 ]; then
                    echo "Error: --min-prs must be a positive integer (got: '$2')"
                    exit 1
                fi
                RETRO_MIN_PRS="$2"
                shift 2
                ;;
            --format)
                if [ -z "${2:-}" ]; then
                    echo "Error: --format requires a type (claude-md, pr-template, checklist)"
                    exit 1
                fi
                case "$2" in
                    claude-md|pr-template|checklist)
                        RETRO_FORMAT="$2"
                        ;;
                    *)
                        echo "Error: --format must be one of: claude-md, pr-template, checklist"
                        exit 1
                        ;;
                esac
                shift 2
                ;;
            --json)
                RETRO_OUTPUT_FORMAT="json"
                shift
                ;;
            --markdown)
                RETRO_OUTPUT_FORMAT="markdown"
                shift
                ;;
            *)
                echo "Unknown option: $1"
                echo "Run 'gh pr-enrich retrospective --help' for usage"
                exit 1
                ;;
        esac
    done

    # Validate reports directory exists
    if [ ! -d "$RETRO_REPORTS_DIR" ]; then
        echo "Error: Reports directory not found: $RETRO_REPORTS_DIR"
        echo "Run 'gh pr-enrich <PR_NUMBER> --enrich' to generate PR reports first."
        exit 1
    fi

    # Check Claude CLI if enrichment requested (after retro_log is defined)
    check_claude_cli() {
        if [ "$RETRO_ENRICH" = true ]; then
            if ! command -v claude &> /dev/null; then
                echo "Warning: Claude CLI not found. Skipping enrichment." >&2
                echo "Install from: https://claude.ai/code" >&2
                RETRO_ENRICH=false
            fi
        fi
    }

    # ============================================================================
    # Retrospective Helper Functions
    # ============================================================================

    # Convert relative date (30d, 2w) to ISO 8601 date
    # Args: $1 - date string (ISO 8601 or relative like "30d", "2w")
    # Output: ISO 8601 date string
    parse_since_date() {
        local input="$1"

        # If already ISO 8601 format, return as-is
        if [[ "$input" =~ ^[0-9]{4}-[0-9]{2}-[0-9]{2} ]]; then
            echo "$input"
            return 0
        fi

        # Parse relative format (e.g., 30d, 2w)
        local num unit
        if [[ "$input" =~ ^([0-9]+)([dwm])$ ]]; then
            num="${BASH_REMATCH[1]}"
            unit="${BASH_REMATCH[2]}"

            case "$unit" in
                d) # days
                    if [[ "$OSTYPE" == "darwin"* ]]; then
                        date -v-${num}d +%Y-%m-%d
                    else
                        date -d "$num days ago" +%Y-%m-%d
                    fi
                    ;;
                w) # weeks
                    local days=$((num * 7))
                    if [[ "$OSTYPE" == "darwin"* ]]; then
                        date -v-${days}d +%Y-%m-%d
                    else
                        date -d "$days days ago" +%Y-%m-%d
                    fi
                    ;;
                m) # months
                    if [[ "$OSTYPE" == "darwin"* ]]; then
                        date -v-${num}m +%Y-%m-%d
                    else
                        date -d "$num months ago" +%Y-%m-%d
                    fi
                    ;;
            esac
            return 0
        fi

        echo "Error: Invalid date format '$input'. Use ISO 8601 (2024-01-15) or relative (30d, 2w, 3m)." >&2
        return 1
    }

    # Discover PR reports with optional filtering
    # Sets: PR_REPORTS array with paths to claude-analysis.json files
    discover_pr_reports() {
        local reports_dir="$1"
        local since_date="$2"
        local author_filter="$3"

        PR_REPORTS=()
        local pr_count=0
        local skipped_no_analysis=0
        local skipped_date=0
        local skipped_author=0

        # Find all PR directories
        for pr_dir in "$reports_dir"/pr-*/; do
            [ -d "$pr_dir" ] || continue

            local analysis_file="$pr_dir/claude-analysis.json"
            local summary_file="$pr_dir/pr-summary.json"

            # Skip if no claude-analysis.json
            if [ ! -f "$analysis_file" ]; then
                ((skipped_no_analysis++)) || true
                continue
            fi

            # Apply date filter
            if [ -n "$since_date" ] && [ -f "$summary_file" ]; then
                local pr_date
                pr_date=$(jq -r '.createdAt // .updatedAt // ""' "$summary_file" 2>/dev/null | cut -d'T' -f1)
                if [ -n "$pr_date" ] && [[ "$pr_date" < "$since_date" ]]; then
                    ((skipped_date++)) || true
                    continue
                fi
            fi

            # Apply author filter
            if [ -n "$author_filter" ] && [ -f "$summary_file" ]; then
                local pr_author
                pr_author=$(jq -r '.author.login // ""' "$summary_file" 2>/dev/null)
                # Check if author matches any in comma-separated list
                local author_match=false
                IFS=',' read -ra AUTHORS <<< "$author_filter"
                for auth in "${AUTHORS[@]}"; do
                    auth=$(echo "$auth" | xargs)  # trim whitespace
                    if [ "$pr_author" = "$auth" ]; then
                        author_match=true
                        break
                    fi
                done
                if [ "$author_match" = false ]; then
                    ((skipped_author++)) || true
                    continue
                fi
            fi

            PR_REPORTS+=("$analysis_file")
            ((pr_count++)) || true
        done

        echo "Found $pr_count PR reports with Claude analysis" >&2
        [ "$skipped_no_analysis" -gt 0 ] && echo "  Skipped $skipped_no_analysis PRs without claude-analysis.json" >&2 || true
        [ "$skipped_date" -gt 0 ] && echo "  Skipped $skipped_date PRs before $since_date" >&2 || true
        [ "$skipped_author" -gt 0 ] && echo "  Skipped $skipped_author PRs by other authors" >&2 || true
    }

    # Aggregate data from all PR reports
    # Args: PR_REPORTS array (global)
    # Output: Aggregated JSON to stdout
    aggregate_pr_data() {
        if [ ${#PR_REPORTS[@]} -eq 0 ]; then
            echo '{"prs": [], "total_prs": 0}'
            return
        fi

        # Combine all analysis files with PR metadata
        local combined="[]"
        for report in "${PR_REPORTS[@]}"; do
            local pr_dir
            pr_dir=$(dirname "$report")
            local pr_num
            pr_num=$(basename "$pr_dir" | sed 's/pr-//')
            local summary_file="$pr_dir/pr-summary.json"

            local pr_meta="{}"
            if [ -f "$summary_file" ]; then
                pr_meta=$(jq -c '{
                    number: .number,
                    title: .title,
                    author: .author.login,
                    created_at: .createdAt,
                    files: [.files[].path]
                }' "$summary_file" 2>/dev/null || echo '{}')
            fi

            # Merge analysis with metadata
            local pr_data
            pr_data=$(jq -c --argjson meta "$pr_meta" '. + {pr_meta: $meta}' "$report" 2>/dev/null)

            combined=$(echo "$combined" | jq -c --argjson pr "$pr_data" '. + [$pr]')
        done

        # Output aggregated structure
        jq -n --argjson prs "$combined" '{
            prs: $prs,
            total_prs: ($prs | length)
        }'
    }

    # Calculate cross-PR patterns (issues that repeat across PRs)
    # Args: $1 - aggregated data JSON
    # Output: Patterns JSON to stdout
    calculate_cross_pr_patterns() {
        local aggregated="$1"

        echo "$aggregated" | jq '
            # Extract all systemic issues with PR context
            [.prs[] |
                .pr_meta.number as $pr_num |
                (.systemic_issues // [])[] |
                {pattern: .pattern, pr: $pr_num, recommendation: .recommendation}
            ] |
            # Group by pattern (normalize to lowercase for matching)
            group_by(.pattern | ascii_downcase) |
            # Create pattern summary
            map({
                pattern: .[0].pattern,
                occurrences: length,
                pr_numbers: [.[].pr] | unique,
                severity: (if length >= 3 then "high" elif length >= 2 then "medium" else "low" end),
                prevention_strategy: .[0].recommendation
            }) |
            # Sort by occurrences descending
            sort_by(-.occurrences)
        '
    }

    # Calculate issue/task category hotspots
    # Args: $1 - aggregated data JSON
    # Output: Hotspots JSON to stdout
    calculate_category_hotspots() {
        local aggregated="$1"

        echo "$aggregated" | jq '
            # Collect all issue-category and task-category references across PRs
            [.prs[] |
                .pr_meta.number as $pr_num |
                (
                    # From issue categories
                    ((.issue_categories // [])[] | {
                        category: .name,
                        severity: .severity,
                        pr: $pr_num
                    }),
                    # From task list
                    ((.task_list // [])[] | {
                        category: "task",
                        severity: .priority,
                        pr: $pr_num
                    })
                )
            ] |
            # Group by category
            group_by(.category) |
            map({
                category: .[0].category,
                issue_count: length,
                prs: [.[].pr] | unique,
                severities: (group_by(.severity) | map({severity: .[0].severity, count: length}))
            }) |
            sort_by(-.issue_count)
        '
    }

    # Generate guiding questions from patterns
    # Args: $1 - patterns JSON, $2 - hotspots JSON
    # Output: Questions checklist to stdout
    generate_guiding_questions() {
        local patterns="$1"
        local hotspots="$2"

        # Combine patterns and hotspots to generate questions
        jq -n --argjson patterns "$patterns" --argjson hotspots "$hotspots" '
        {
            before_implementation: [
                ($patterns[] | select(.occurrences >= 2) |
                    "Have I addressed the pattern: \(.pattern)?"
                ),
                ($hotspots[:5][] |
                    "Have I checked \(.category) for similar issues?"
                )
            ] | unique,

            before_review: [
                ($patterns[] | select(.severity == "high") |
                    "Does this change avoid: \(.pattern)?"
                )
            ] | unique,

            after_review: [
                "Have I updated the team on systemic patterns found?",
                "Should this pattern be added to our documentation?"
            ]
        }'
    }

    # Generate improvement tracking
    # Args: $1 - aggregated data JSON
    # Output: Tracking JSON to stdout
    generate_improvement_tracking() {
        local aggregated="$1"

        echo "$aggregated" | jq '
            # Count process improvements suggested
            {
                suggestions_made: [.prs[] | (.process_improvements // []) | length] | add,
                by_category: (
                    [.prs[] | (.process_improvements // [])[]] |
                    group_by(.category) |
                    map({category: .[0].category, count: length})
                ),
                pr_template_suggestions: [.prs[] | (.pr_template_suggestions // []) | length] | add
            }
        '
    }

    # Generate team retrospective summary
    # Args: $1 - aggregated data, $2 - patterns, $3 - hotspots
    # Output: Summary JSON to stdout
    generate_retro_summary() {
        local aggregated="$1"
        local patterns="$2"
        local hotspots="$3"

        jq -n \
            --argjson agg "$aggregated" \
            --argjson patterns "$patterns" \
            --argjson hotspots "$hotspots" '
        {
            overview: {
                total_prs_analyzed: $agg.total_prs,
                total_issues: [$agg.prs[] | (.issue_categories // []) | length] | add,
                total_tasks: [$agg.prs[] | (.task_list // []) | length] | add,
                total_patterns: ($patterns | length)
            },
            top_issue_categories: (
                [$agg.prs[] | (.issue_categories // [])[]] |
                group_by(.name) |
                map({name: .[0].name, count: length, severities: (group_by(.severity) | map({s: .[0].severity, c: length}))}) |
                sort_by(-.count) |
                .[0:5]
            ),
            recurring_patterns: ($patterns | .[0:5]),
            hotspot_areas: ($hotspots | .[0:5]),
            recommended_actions: [
                ($patterns[:3][] | {
                    action: .prevention_strategy,
                    effort: "medium",
                    impact: (if .occurrences >= 3 then "high" else "medium" end),
                    addresses_prs: .pr_numbers
                })
            ]
        }'
    }

    # Claude meta-analysis schema for retrospectives
    RETRO_CLAUDE_SCHEMA='{
      "type": "object",
      "properties": {
        "meta_patterns": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "root_cause": { "type": "string" },
              "manifestations": { "type": "array", "items": { "type": "string" } },
              "prevention_strategy": { "type": "string" },
              "implementation_steps": { "type": "array", "items": { "type": "string" } }
            },
            "required": ["root_cause", "manifestations", "prevention_strategy"]
          }
        },
        "knowledge_gaps": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "topic": { "type": "string" },
              "evidence": { "type": "array", "items": { "type": "string" } },
              "learning_resources": { "type": "array", "items": { "type": "string" } }
            },
            "required": ["topic", "evidence"]
          }
        },
        "automation_opportunities": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "tool_or_check": { "type": "string" },
              "issues_caught": { "type": "array", "items": { "type": "string" } },
              "implementation_hint": { "type": "string" }
            },
            "required": ["tool_or_check", "issues_caught"]
          }
        },
        "guiding_questions_refined": {
          "type": "object",
          "properties": {
            "planning_phase": { "type": "array", "items": { "type": "string" } },
            "implementation_phase": { "type": "array", "items": { "type": "string" } },
            "review_phase": { "type": "array", "items": { "type": "string" } }
          }
        },
        "claude_md_additions": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "section": { "type": "string" },
              "content": { "type": "string" }
            },
            "required": ["section", "content"]
          }
        },
        "pr_template_additions": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "section": { "type": "string" },
              "checkbox_or_question": { "type": "string" },
              "rationale": { "type": "string" }
            },
            "required": ["section", "checkbox_or_question", "rationale"]
          }
        }
      },
      "required": ["meta_patterns", "knowledge_gaps", "automation_opportunities", "guiding_questions_refined", "claude_md_additions", "pr_template_additions"]
    }'

    # Run Claude meta-analysis on aggregated retrospective data
    run_retrospective_claude_analysis() {
        local context_file="$1"
        local output_file="$2"

        local system_prompt="You are analyzing aggregated data from multiple PR reviews to identify meta-patterns, knowledge gaps, and opportunities for improvement.

Your task is to:
1. Identify root causes that explain multiple recurring patterns
2. Highlight knowledge gaps the team should address
3. Suggest automation (linting, CI checks) that would catch these issues
4. Refine guiding questions to be specific and actionable
5. Propose additions to CLAUDE.md and PR templates

Focus on actionable, specific insights. Prioritize items that would have the highest impact."

        local user_prompt="Analyze this aggregated PR retrospective data and provide meta-analysis:

$(cat "$context_file")"

        claude --print \
            --model sonnet \
            --output-format json \
            --json-schema "$RETRO_CLAUDE_SCHEMA" \
            --system-prompt "$system_prompt" \
            "$user_prompt" > "$output_file"

        return $?
    }

    # Generate markdown report from retrospective data
    generate_retrospective_markdown() {
        local summary="$1"
        local patterns="$2"
        local hotspots="$3"
        local questions="$4"
        local tracking="$5"
        local claude_analysis="${6:-}"

        local timestamp
        timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

        cat << EOF
# Team Retrospective Report

**Generated:** $timestamp

## Overview

$(echo "$summary" | jq -r '
"- **PRs Analyzed:** \(.overview.total_prs_analyzed)
- **Total Issues Found:** \(.overview.total_issues // 0)
- **Total Tasks Generated:** \(.overview.total_tasks // 0)
- **Recurring Patterns:** \(.overview.total_patterns)"
')

## Top Issue Categories

$(echo "$summary" | jq -r '
if (.top_issue_categories | length) == 0 then
    "_No issue categories found._"
else
    .top_issue_categories[] |
    "- **\(.name)** (\(.count) occurrences)"
end
')

## Cross-PR Systemic Patterns

$(echo "$patterns" | jq -r '
if length == 0 then
    "_No recurring patterns identified._"
else
    .[] |
    "### \(.pattern)

**Occurrences:** \(.occurrences) | **Severity:** \(.severity)
**PRs:** \(.pr_numbers | map("#\(.)") | join(", "))

**Prevention Strategy:** \(.prevention_strategy // "N/A")
"
end
')

## Component Hotspots

$(echo "$hotspots" | jq -r '
if length == 0 then
    "_No hotspots identified._"
else
    .[:10][] |
    "- **\(.category)** - \(.issue_count) issues across \(.prs | length) PRs"
end
')

## Guiding Questions Checklist

### Before Implementation
$(echo "$questions" | jq -r '
if (.before_implementation | length) == 0 then
    "_No questions generated._"
else
    .before_implementation[] | "- [ ] \(.)"
end
')

### Before Review
$(echo "$questions" | jq -r '
if (.before_review | length) == 0 then
    "_No questions generated._"
else
    .before_review[] | "- [ ] \(.)"
end
')

### After Review
$(echo "$questions" | jq -r '.after_review[] | "- [ ] \(.)"')

## Improvement Tracking

$(echo "$tracking" | jq -r '
"- **Total Suggestions Made:** \(.suggestions_made // 0)
- **PR Template Suggestions:** \(.pr_template_suggestions // 0)

### By Category
" + (if (.by_category | length) == 0 then "_None_" else (.by_category[] | "- \(.category): \(.count)") | . end)
')

## Recommended Actions

$(echo "$summary" | jq -r '
if (.recommended_actions | length) == 0 then
    "_No recommended actions._"
else
    .recommended_actions[] |
    "### \(.action)
**Effort:** \(.effort) | **Impact:** \(.impact)
**Addresses PRs:** \(.addresses_prs | map("#\(.)") | join(", "))
"
end
')
EOF

        # Add Claude meta-analysis if available
        if [ -n "$claude_analysis" ] && [ -f "$claude_analysis" ]; then
            cat << 'CLAUDE_HEADER'

---

## ü§ñ Claude Meta-Analysis

CLAUDE_HEADER

            jq -r '
"### Root Cause Analysis

" + (if (.meta_patterns | length) == 0 then "_No meta-patterns identified._" else
(.meta_patterns[] |
"#### \(.root_cause)

**Manifestations:**
" + (.manifestations | map("- " + .) | join("\n")) + "

**Prevention:** \(.prevention_strategy)
" + (if .implementation_steps then "**Steps:** " + (.implementation_steps | join(" ‚Üí ")) else "" end))
end) + "

### Knowledge Gaps

" + (if (.knowledge_gaps | length) == 0 then "_No knowledge gaps identified._" else
(.knowledge_gaps[] | "- **\(.topic):** " + (.evidence | join("; ")))
end) + "

### Automation Opportunities

" + (if (.automation_opportunities | length) == 0 then "_No automation opportunities identified._" else
(.automation_opportunities[] | "- **\(.tool_or_check):** Would catch " + (.issues_caught | join(", ")))
end)
' "$claude_analysis"
        fi

        cat << 'FOOTER'

---

*Report generated by gh-pr-enrich retrospective*
FOOTER
    }

    # Generate special format outputs
    generate_format_output() {
        local format="$1"
        local questions="$2"
        local summary="$3"
        local claude_analysis="${4:-}"

        case "$format" in
            claude-md)
                cat << 'CLAUDE_MD_HEADER'
## Lessons Learned from PR Reviews

<!-- Add this section to your CLAUDE.md -->

### Common Patterns to Avoid

CLAUDE_MD_HEADER
                echo "$summary" | jq -r '
                    .recurring_patterns[:5][] |
                    "- **\(.pattern)**: \(.prevention_strategy // "Address proactively")"
                '

                if [ -n "$claude_analysis" ] && [ -f "$claude_analysis" ]; then
                    echo ""
                    echo "### Implementation Guidelines"
                    jq -r '.claude_md_additions[] | "#### \(.section)\n\(.content)\n"' "$claude_analysis"
                fi
                ;;

            pr-template)
                cat << 'PR_TEMPLATE_HEADER'
## PR Review Checklist

<!-- Add these items to your PR template -->

PR_TEMPLATE_HEADER
                echo "$questions" | jq -r '
                    "### Before Submitting\n" +
                    (.before_implementation | map("- [ ] " + .) | join("\n")) +
                    "\n\n### Before Requesting Review\n" +
                    (.before_review | map("- [ ] " + .) | join("\n"))
                '

                if [ -n "$claude_analysis" ] && [ -f "$claude_analysis" ]; then
                    echo ""
                    echo "### Additional Checks"
                    jq -r '.pr_template_additions[] | "- [ ] \(.checkbox_or_question)\n  _(\(.rationale))_"' "$claude_analysis"
                fi
                ;;

            checklist)
                cat << 'CHECKLIST_HEADER'
# Implementation Checklist

<!-- Standalone checklist derived from PR review patterns -->

CHECKLIST_HEADER
                echo "$questions" | jq -r '
                    "## Planning Phase\n" +
                    (.before_implementation | map("- [ ] " + .) | join("\n")) +
                    "\n\n## Implementation Phase\n" +
                    (.before_review | map("- [ ] " + .) | join("\n")) +
                    "\n\n## Review Phase\n" +
                    (.after_review | map("- [ ] " + .) | join("\n"))
                '

                if [ -n "$claude_analysis" ] && [ -f "$claude_analysis" ]; then
                    echo ""
                    echo "## Refined Questions from Analysis"
                    jq -r '
                        "### Planning\n" +
                        (.guiding_questions_refined.planning_phase // [] | map("- [ ] " + .) | join("\n")) +
                        "\n\n### Implementation\n" +
                        (.guiding_questions_refined.implementation_phase // [] | map("- [ ] " + .) | join("\n")) +
                        "\n\n### Review\n" +
                        (.guiding_questions_refined.review_phase // [] | map("- [ ] " + .) | join("\n"))
                    ' "$claude_analysis"
                fi
                ;;
        esac
    }

    # ============================================================================
    # Main Retrospective Logic
    # ============================================================================

    # Helper to print progress messages (to stderr for --json/--markdown modes)
    retro_log() {
        if [ "$RETRO_OUTPUT_FORMAT" = "json" ] || [ "$RETRO_OUTPUT_FORMAT" = "markdown" ]; then
            echo "$1" >&2
        else
            echo "$1"
        fi
    }

    # Check Claude CLI availability (now that retro_log is defined)
    check_claude_cli

    retro_log "Running retrospective analysis..."

    # Parse since date if provided
    if [ -n "$RETRO_SINCE" ]; then
        RETRO_SINCE=$(parse_since_date "$RETRO_SINCE") || exit 1
        retro_log "Filtering PRs since: $RETRO_SINCE"
    fi

    # Discover PR reports
    discover_pr_reports "$RETRO_REPORTS_DIR" "$RETRO_SINCE" "$RETRO_AUTHOR"

    # Check minimum PRs requirement
    if [ ${#PR_REPORTS[@]} -eq 0 ]; then
        echo "Error: No PR reports found with Claude analysis." >&2
        echo "Run 'gh pr-enrich <PR_NUMBER> --enrich' to generate PR reports first." >&2
        exit 1
    fi

    if [ ${#PR_REPORTS[@]} -lt "$RETRO_MIN_PRS" ]; then
        retro_log "Warning: Found ${#PR_REPORTS[@]} PR(s), but minimum is $RETRO_MIN_PRS."
        retro_log "Use --min-prs to lower the threshold, or generate more PR reports."
        retro_log "Continuing with available data..."
    fi

    # Create output directory
    mkdir -p "$RETRO_OUTPUT_DIR"

    TIMESTAMP=$(date -u +"%Y%m%d-%H%M%S")

    # Aggregate data
    retro_log "Aggregating PR data..."
    AGGREGATED=$(aggregate_pr_data)
    echo "$AGGREGATED" > "$RETRO_OUTPUT_DIR/aggregated-data.json"

    # Calculate patterns
    retro_log "Calculating cross-PR patterns..."
    PATTERNS=$(calculate_cross_pr_patterns "$AGGREGATED")
    echo "$PATTERNS" > "$RETRO_OUTPUT_DIR/cross-pr-patterns.json"

    # Calculate hotspots
    retro_log "Calculating hotspots..."
    HOTSPOTS=$(calculate_category_hotspots "$AGGREGATED")
    echo "$HOTSPOTS" > "$RETRO_OUTPUT_DIR/hotspots.json"

    # Generate guiding questions
    retro_log "Generating guiding questions..."
    QUESTIONS=$(generate_guiding_questions "$PATTERNS" "$HOTSPOTS")
    echo "$QUESTIONS" > "$RETRO_OUTPUT_DIR/guiding-questions.json"

    # Generate improvement tracking
    retro_log "Generating improvement tracking..."
    TRACKING=$(generate_improvement_tracking "$AGGREGATED")
    echo "$TRACKING" > "$RETRO_OUTPUT_DIR/improvement-tracking.json"

    # Generate summary
    retro_log "Generating summary..."
    SUMMARY=$(generate_retro_summary "$AGGREGATED" "$PATTERNS" "$HOTSPOTS")
    echo "$SUMMARY" > "$RETRO_OUTPUT_DIR/summary.json"

    # Optional Claude enrichment
    CLAUDE_ANALYSIS_FILE=""
    if [ "$RETRO_ENRICH" = true ]; then
        retro_log "Running Claude meta-analysis..."

        # Build context for Claude
        jq -n \
            --argjson summary "$SUMMARY" \
            --argjson patterns "$PATTERNS" \
            --argjson hotspots "$HOTSPOTS" \
            --argjson tracking "$TRACKING" \
            '{
                summary: $summary,
                patterns: $patterns,
                hotspots: $hotspots,
                tracking: $tracking
            }' > "$RETRO_OUTPUT_DIR/claude-context.json"

        if run_retrospective_claude_analysis "$RETRO_OUTPUT_DIR/claude-context.json" "$RETRO_OUTPUT_DIR/claude-raw-response.json"; then
            retro_log "‚úÖ Claude meta-analysis complete"

            if jq -e '.structured_output' "$RETRO_OUTPUT_DIR/claude-raw-response.json" > /dev/null 2>&1; then
                jq '.structured_output' "$RETRO_OUTPUT_DIR/claude-raw-response.json" > "$RETRO_OUTPUT_DIR/claude-meta-analysis.json"
            else
                cp "$RETRO_OUTPUT_DIR/claude-raw-response.json" "$RETRO_OUTPUT_DIR/claude-meta-analysis.json"
            fi
            CLAUDE_ANALYSIS_FILE="$RETRO_OUTPUT_DIR/claude-meta-analysis.json"
        else
            retro_log "‚ö†Ô∏è  Claude meta-analysis failed. Continuing without enrichment."
        fi
    fi

    # Generate markdown report
    retro_log "Generating markdown report..."
    generate_retrospective_markdown "$SUMMARY" "$PATTERNS" "$HOTSPOTS" "$QUESTIONS" "$TRACKING" "$CLAUDE_ANALYSIS_FILE" \
        > "$RETRO_OUTPUT_DIR/retrospective-report.md"

    # Generate combined JSON
    jq -n \
        --argjson summary "$SUMMARY" \
        --argjson patterns "$PATTERNS" \
        --argjson hotspots "$HOTSPOTS" \
        --argjson questions "$QUESTIONS" \
        --argjson tracking "$TRACKING" \
        --arg timestamp "$TIMESTAMP" \
        --arg version "$VERSION" \
        '{
            metadata: {
                generated_at: $timestamp,
                version: $version
            },
            summary: $summary,
            cross_pr_patterns: $patterns,
            hotspots: $hotspots,
            guiding_questions: $questions,
            improvement_tracking: $tracking
        }' > "$RETRO_OUTPUT_DIR/retrospective-data.json"

    # Add Claude analysis to combined JSON if available
    if [ -n "$CLAUDE_ANALYSIS_FILE" ] && [ -f "$CLAUDE_ANALYSIS_FILE" ]; then
        jq --argjson claude "$(cat "$CLAUDE_ANALYSIS_FILE")" \
            '. + {claude_meta_analysis: $claude}' \
            "$RETRO_OUTPUT_DIR/retrospective-data.json" > "$RETRO_OUTPUT_DIR/retrospective-data.tmp.json" && \
            mv "$RETRO_OUTPUT_DIR/retrospective-data.tmp.json" "$RETRO_OUTPUT_DIR/retrospective-data.json"
    fi

    # Handle special format output
    if [ -n "$RETRO_FORMAT" ]; then
        echo ""
        echo "=== ${RETRO_FORMAT} output ==="
        echo ""
        generate_format_output "$RETRO_FORMAT" "$QUESTIONS" "$SUMMARY" "$CLAUDE_ANALYSIS_FILE"
        exit 0
    fi

    # Standard output
    case $RETRO_OUTPUT_FORMAT in
        "json")
            cat "$RETRO_OUTPUT_DIR/retrospective-data.json"
            ;;
        "markdown")
            cat "$RETRO_OUTPUT_DIR/retrospective-report.md"
            ;;
        "combined")
            echo "‚úÖ Retrospective analysis complete!"
            echo "üìÅ Directory: $RETRO_OUTPUT_DIR"
            echo "üìä Markdown: $RETRO_OUTPUT_DIR/retrospective-report.md"
            echo "ü§ñ JSON: $RETRO_OUTPUT_DIR/retrospective-data.json"
            if [ -n "$CLAUDE_ANALYSIS_FILE" ]; then
                echo "üß† Claude Analysis: $RETRO_OUTPUT_DIR/claude-meta-analysis.json"
            fi
            echo ""
            echo "Quick stats:"
            echo "$SUMMARY" | jq -r '"  PRs analyzed: \(.overview.total_prs_analyzed)"'
            echo "$PATTERNS" | jq -r '"  Recurring patterns: \(length)"'
            ;;
    esac

    exit 0
fi

# Handle uninstall-skill subcommand
if [ "$1" = "uninstall-skill" ]; then
    SKILL_TARGET="$HOME/.claude/skills/gh-pr-enrich"

    if [ -L "$SKILL_TARGET" ]; then
        if ! rm "$SKILL_TARGET" 2>/dev/null; then
            echo "Error: Could not remove symlink at $SKILL_TARGET"
            echo "Check permissions"
            exit 1
        fi
        echo "‚úÖ Claude skill uninstalled"
    elif [ -d "$SKILL_TARGET" ]; then
        echo "‚ö†Ô∏è  $SKILL_TARGET is a directory, not a symlink"
        echo "   Remove manually if desired: rm -rf $SKILL_TARGET"
        exit 1
    elif [ -f "$SKILL_TARGET" ]; then
        echo "‚ö†Ô∏è  $SKILL_TARGET is a regular file, not a symlink"
        echo "   Remove manually if desired: rm $SKILL_TARGET"
        exit 1
    else
        echo "‚ÑπÔ∏è  Skill not installed at $SKILL_TARGET"
    fi
    exit 0
fi

# Handle resolve subcommand - resolve PR review threads by ID
if [ "$1" = "resolve" ]; then
    shift
    if [ $# -eq 0 ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        echo "Usage: gh pr-enrich resolve <THREAD_ID> [THREAD_ID...]"
        echo ""
        echo "Resolve one or more PR review threads by their GraphQL ID."
        echo "Thread IDs start with 'PRRT_' and can be found in:"
        echo "  - claude-analysis.json (task_list[].thread_ids)"
        echo "  - comment-threads.json"
        echo ""
        echo "Example:"
        echo "  gh pr-enrich resolve PRRT_kwDORDoRf85rizQq PRRT_kwDORDoRf85rizQ3"
        exit 0
    fi

    RESOLVED=0
    FAILED=0

    for THREAD_ID in "$@"; do
        # Validate thread ID format (PRRT_ prefix + base64-safe characters)
        if [[ ! "$THREAD_ID" =~ ^PRRT_[A-Za-z0-9_-]+$ ]]; then
            echo "‚ö†Ô∏è  Skipping invalid thread ID: $THREAD_ID (must start with PRRT_ followed by valid characters)"
            ((FAILED++))
            continue
        fi

        # Resolve the thread via GraphQL (using parameterized query to prevent injection)
        RESULT=$(gh api graphql \
            -F threadId="$THREAD_ID" \
            -f query='mutation($threadId: ID!) {
                resolveReviewThread(input: {threadId: $threadId}) {
                    thread { isResolved id }
                }
            }' 2>&1)

        if echo "$RESULT" | jq -e '.data.resolveReviewThread.thread.isResolved' > /dev/null 2>&1; then
            echo "‚úÖ Resolved: $THREAD_ID"
            ((RESOLVED++))
        else
            ERROR=$(echo "$RESULT" | jq -r '.errors[0].message // "Unknown error"' 2>/dev/null || echo "$RESULT")
            echo "‚ùå Failed to resolve $THREAD_ID: $ERROR"
            ((FAILED++))
        fi
    done

    echo ""
    echo "Summary: $RESOLVED resolved, $FAILED failed"
    exit 0
fi

# Handle watch subcommand - monitor PR for new comments
if [ "$1" = "watch" ]; then
    shift
    if [ $# -eq 0 ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        echo "Usage: gh pr-enrich watch <PR_NUMBER> [OPTIONS]"
        echo ""
        echo "Monitor a PR for new comments and optionally run analysis."
        echo ""
        echo "Options:"
        echo "  --interval MIN   Check interval in minutes (default: 5)"
        echo "  --enrich         Run Claude analysis when new comments found"
        echo "  --notify         Show desktop notification (macOS only)"
        echo ""
        echo "Example:"
        echo "  gh pr-enrich watch 123 --interval 2 --enrich"
        echo ""
        echo "Press Ctrl+C to stop watching."
        exit 0
    fi

    WATCH_PR="$1"
    shift

    # Validate PR number
    if ! [[ "$WATCH_PR" =~ ^[0-9]+$ ]]; then
        echo "Error: PR number must be a positive integer"
        exit 1
    fi

    WATCH_INTERVAL=5
    WATCH_ENRICH=false
    WATCH_NOTIFY=false

    while [[ $# -gt 0 ]]; do
        case $1 in
            --interval)
                if [ -z "${2:-}" ]; then
                    echo "Error: --interval requires a numeric value"
                    exit 1
                fi
                if ! [[ "$2" =~ ^[0-9]+$ ]] || [ "$2" -lt 1 ]; then
                    echo "Error: --interval must be a positive integer (got: '$2')"
                    exit 1
                fi
                WATCH_INTERVAL="$2"
                shift 2
                ;;
            --enrich)
                WATCH_ENRICH=true
                shift
                ;;
            --notify)
                WATCH_NOTIFY=true
                shift
                ;;
            *)
                echo "Unknown option: $1"
                exit 1
                ;;
        esac
    done

    # Validate repository access
    REPO=$(gh repo view --json nameWithOwner -q .nameWithOwner 2>/dev/null)
    if [ -z "$REPO" ]; then
        echo "Error: Could not determine repository. Are you in a git repo with a GitHub remote?"
        exit 1
    fi

    # Verify PR exists before starting watch loop
    if ! gh pr view "$WATCH_PR" --json number > /dev/null 2>&1; then
        echo "Error: PR #$WATCH_PR not found in $REPO"
        echo "Verify the PR number and that you have access to the repository."
        exit 1
    fi

    echo "üëÄ Watching PR #$WATCH_PR in $REPO"
    echo "   Checking every $WATCH_INTERVAL minute(s)"
    echo "   Enrich: $WATCH_ENRICH | Notify: $WATCH_NOTIFY"
    echo "   Press Ctrl+C to stop"
    echo ""

    # Split repo into owner and name for parameterized queries
    REPO_OWNER="${REPO%/*}"
    REPO_NAME="${REPO#*/}"

    # Get initial comment count
    LAST_COUNT=$(gh pr view "$WATCH_PR" --json comments,reviews --jq '(.comments | length) + (.reviews | length)' 2>/dev/null || echo "0")
    # Note: Limited to 100 review threads. PRs with more threads may show incomplete counts.
    LAST_UNRESOLVED=$(gh api graphql \
        -F owner="$REPO_OWNER" \
        -F name="$REPO_NAME" \
        -F prNumber="$WATCH_PR" \
        -f query='query($owner: String!, $name: String!, $prNumber: Int!) {
            repository(owner: $owner, name: $name) {
                pullRequest(number: $prNumber) {
                    reviewThreads(first: 100) {
                        nodes { isResolved }
                    }
                }
            }
        }' --jq '[.data.repository.pullRequest.reviewThreads.nodes[] | select(.isResolved == false)] | length' 2>/dev/null || echo "0")

    echo "üìä Initial state: $LAST_COUNT comments/reviews, $LAST_UNRESOLVED unresolved threads"
    echo ""

    CONSECUTIVE_FAILURES=0
    MAX_FAILURES=3

    while true; do
        sleep $((WATCH_INTERVAL * 60))

        # Get current counts (using parameterized queries to prevent injection)
        CURRENT_COUNT=$(gh pr view "$WATCH_PR" --json comments,reviews --jq '(.comments | length) + (.reviews | length)' 2>/dev/null)
        CURRENT_UNRESOLVED=$(gh api graphql \
            -F owner="$REPO_OWNER" \
            -F name="$REPO_NAME" \
            -F prNumber="$WATCH_PR" \
            -f query='query($owner: String!, $name: String!, $prNumber: Int!) {
                repository(owner: $owner, name: $name) {
                    pullRequest(number: $prNumber) {
                        reviewThreads(first: 100) {
                            nodes { isResolved }
                        }
                    }
                }
            }' --jq '[.data.repository.pullRequest.reviewThreads.nodes[] | select(.isResolved == false)] | length' 2>/dev/null)

        TIMESTAMP=$(date '+%H:%M:%S')

        # Check for API failures
        if [ -z "$CURRENT_COUNT" ] || [ -z "$CURRENT_UNRESOLVED" ]; then
            ((CONSECUTIVE_FAILURES++))
            echo "[$TIMESTAMP] ‚ö†Ô∏è  API request failed (attempt $CONSECUTIVE_FAILURES/$MAX_FAILURES)"
            if [ "$CONSECUTIVE_FAILURES" -ge "$MAX_FAILURES" ]; then
                echo "[$TIMESTAMP] ‚ùå Too many consecutive failures. Check network/auth and restart watch."
                exit 1
            fi
            continue
        fi
        CONSECUTIVE_FAILURES=0

        if [ "$CURRENT_COUNT" -ne "$LAST_COUNT" ] || [ "$CURRENT_UNRESOLVED" -ne "$LAST_UNRESOLVED" ]; then
            NEW_COMMENTS=$((CURRENT_COUNT - LAST_COUNT))
            THREAD_DIFF=$((CURRENT_UNRESOLVED - LAST_UNRESOLVED))

            echo "[$TIMESTAMP] üîî Changes detected!"
            [ "$NEW_COMMENTS" -ne 0 ] && echo "   New comments/reviews: $NEW_COMMENTS"
            [ "$THREAD_DIFF" -gt 0 ] && echo "   New unresolved threads: +$THREAD_DIFF"
            [ "$THREAD_DIFF" -lt 0 ] && echo "   Threads resolved: ${THREAD_DIFF#-}"

            # Desktop notification (macOS)
            if [ "$WATCH_NOTIFY" = true ] && command -v osascript &> /dev/null; then
                osascript -e "display notification \"${NEW_COMMENTS} new comments, ${CURRENT_UNRESOLVED} unresolved\" with title \"PR #${WATCH_PR} Updated\"" 2>/dev/null || true
            fi

            # Run enrichment if requested
            if [ "$WATCH_ENRICH" = true ] && [ "$CURRENT_UNRESOLVED" -gt 0 ]; then
                echo "   Running analysis..."
                if "$0" "$WATCH_PR" --enrich --output-dir ".reports/pr-reviews/pr-$WATCH_PR" > /dev/null 2>&1; then
                    if [ -f ".reports/pr-reviews/pr-$WATCH_PR/claude-analysis.md" ]; then
                        echo "   ‚úÖ Analysis updated: .reports/pr-reviews/pr-$WATCH_PR/claude-analysis.md"
                    else
                        echo "   ‚ö†Ô∏è  Analysis ran but no claude-analysis.md created (no unresolved threads or Claude unavailable)"
                    fi
                else
                    echo "   ‚ùå Analysis failed - check manually with: gh pr-enrich $WATCH_PR --enrich"
                fi
            fi

            LAST_COUNT=$CURRENT_COUNT
            LAST_UNRESOLVED=$CURRENT_UNRESOLVED
            echo ""
        else
            echo "[$TIMESTAMP] No changes (comments: $CURRENT_COUNT, unresolved: $CURRENT_UNRESOLVED)"
        fi
    done
fi

# Handle address subcommand - interactive mode to work through issues
if [ "$1" = "address" ]; then
    shift
    if [ $# -eq 0 ] || [ "$1" = "--help" ] || [ "$1" = "-h" ]; then
        echo "Usage: gh pr-enrich address <PR_NUMBER>"
        echo ""
        echo "Interactive mode to work through PR review issues one by one."
        echo "Shows each task from Claude analysis and lets you mark as fixed."
        echo ""
        echo "Requires: Previous 'gh pr-enrich <PR> --enrich' run"
        echo ""
        echo "Controls:"
        echo "  f - Mark as fixed (resolves thread)"
        echo "  s - Skip to next task"
        echo "  o - Open thread in browser"
        echo "  q - Quit"
        exit 0
    fi

    ADDRESS_PR="$1"
    if ! [[ "$ADDRESS_PR" =~ ^[0-9]+$ ]]; then
        echo "Error: PR number must be a positive integer"
        exit 1
    fi

    ANALYSIS_FILE=".reports/pr-reviews/pr-$ADDRESS_PR/claude-analysis.json"

    if [ ! -f "$ANALYSIS_FILE" ]; then
        echo "Error: Analysis not found at $ANALYSIS_FILE"
        echo "Run 'gh pr-enrich $ADDRESS_PR --enrich' first"
        exit 1
    fi

    # Validate JSON structure before use
    if ! jq -e '.task_list' "$ANALYSIS_FILE" > /dev/null 2>&1; then
        echo "Error: Analysis file is missing 'task_list' or contains invalid JSON"
        echo "File: $ANALYSIS_FILE"
        echo "Re-run: gh pr-enrich $ADDRESS_PR --enrich"
        exit 1
    fi

    # Get task count
    TASK_COUNT=$(jq '.task_list | length' "$ANALYSIS_FILE")
    if [ "$TASK_COUNT" -eq 0 ]; then
        echo "‚úÖ No tasks found in analysis. All done!"
        exit 0
    fi

    echo "üìã Found $TASK_COUNT tasks to address"
    echo ""

    CURRENT=0
    FIXED=0
    SKIPPED=0

    while [ $CURRENT -lt $TASK_COUNT ]; do
        TASK=$(jq -r ".task_list[$CURRENT]" "$ANALYSIS_FILE")
        PRIORITY=$(echo "$TASK" | jq -r '.priority')
        DESCRIPTION=$(echo "$TASK" | jq -r '.task')
        THREAD_IDS=$(echo "$TASK" | jq -r '.thread_ids | join(", ")')

        # Color based on priority
        case $PRIORITY in
            critical) PRIORITY_COLOR="\033[1;31m" ;;  # Bold red
            high)     PRIORITY_COLOR="\033[0;31m" ;;  # Red
            medium)   PRIORITY_COLOR="\033[0;33m" ;;  # Yellow
            low)      PRIORITY_COLOR="\033[0;32m" ;;  # Green
            *)        PRIORITY_COLOR="\033[0m" ;;
        esac

        echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
        echo -e "Task $((CURRENT + 1))/$TASK_COUNT [${PRIORITY_COLOR}${PRIORITY}\033[0m]"
        echo ""
        echo "$DESCRIPTION"
        echo ""
        if [ -n "$THREAD_IDS" ] && [ "$THREAD_IDS" != "null" ]; then
            echo "Threads: $THREAD_IDS"
        fi
        echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
        echo ""
        echo -n "[f]ixed  [s]kip  [o]pen in browser  [q]uit > "

        read -r -n 1 ACTION
        echo ""

        case $ACTION in
            f|F)
                # Resolve all threads for this task (using parameterized query to prevent injection)
                THREAD_ARRAY=$(echo "$TASK" | jq -r '.thread_ids[]' 2>/dev/null)
                RESOLVE_FAILED=0
                if [ -n "$THREAD_ARRAY" ]; then
                    for TID in $THREAD_ARRAY; do
                        # Validate thread ID format before API call
                        if [[ ! "$TID" =~ ^PRRT_[A-Za-z0-9_-]+$ ]]; then
                            echo "  ‚ö†Ô∏è  Invalid thread ID format: $TID"
                            ((RESOLVE_FAILED++))
                            continue
                        fi
                        RESULT=$(gh api graphql \
                            -F threadId="$TID" \
                            -f query='mutation($threadId: ID!) {
                                resolveReviewThread(input: {threadId: $threadId}) {
                                    thread { isResolved }
                                }
                            }' 2>&1)
                        if echo "$RESULT" | jq -e '.data.resolveReviewThread.thread.isResolved' > /dev/null 2>&1; then
                            echo "  ‚úÖ Resolved: $TID"
                        else
                            ERROR=$(echo "$RESULT" | jq -r '.errors[0].message // "Unknown error"' 2>/dev/null || echo "$RESULT")
                            echo "  ‚ùå Failed to resolve $TID: $ERROR"
                            ((RESOLVE_FAILED++))
                        fi
                    done
                    if [ "$RESOLVE_FAILED" -gt 0 ]; then
                        echo "  ‚ö†Ô∏è  $RESOLVE_FAILED thread(s) failed to resolve - task may not be fully addressed"
                    fi
                fi
                ((FIXED++))
                ((CURRENT++))
                echo ""
                ;;
            s|S)
                echo "  ‚è≠Ô∏è  Skipped"
                ((SKIPPED++))
                ((CURRENT++))
                echo ""
                ;;
            o|O)
                # Get first thread URL
                FIRST_TID=$(echo "$TASK" | jq -r '.thread_ids[0] // empty')
                if [ -n "$FIRST_TID" ]; then
                    # Find URL in comment threads
                    THREAD_FILE=".reports/pr-reviews/pr-$ADDRESS_PR/comment-threads.json"
                    if [ -f "$THREAD_FILE" ]; then
                        URL=$(jq -r ".data.repository.pullRequest.reviewThreads.nodes[] | select(.id == \"$FIRST_TID\") | .comments.nodes[0].url // empty" "$THREAD_FILE")
                        if [ -n "$URL" ]; then
                            echo "  üåê Opening: $URL"
                            # Try macOS 'open', then Linux 'xdg-open'; Windows not supported
                            open "$URL" 2>/dev/null || xdg-open "$URL" 2>/dev/null || echo "  Could not open browser (try manually: $URL)"
                        else
                            echo "  Could not find URL for thread $FIRST_TID"
                        fi
                    else
                        echo "  Thread details file not found: $THREAD_FILE"
                        echo "  Re-run: gh pr-enrich $ADDRESS_PR to refresh data"
                    fi
                else
                    echo "  No thread ID associated with this task"
                fi
                # Don't advance - let user decide after viewing
                ;;
            q|Q)
                echo ""
                echo "Quitting..."
                break
                ;;
            *)
                echo "  Unknown action. Use f/s/o/q"
                ;;
        esac
    done

    echo ""
    echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
    echo "Summary: $FIXED fixed, $SKIPPED skipped, $((TASK_COUNT - CURRENT)) remaining"
    exit 0
fi

# Handle --help flag
if [ "$1" = "--help" ] || [ "$1" = "-h" ] || [ $# -lt 1 ]; then
    cat << 'HELP'
gh pr-enrich - Comprehensive PR analysis with optional Claude AI enrichment

USAGE:
  gh pr-enrich <PR_NUMBER> [OPTIONS]
  gh pr-enrich <SUBCOMMAND> [ARGS]

SUBCOMMANDS:
  install-skill       Install Claude Code skill (symlink to ~/.claude/skills/)
  uninstall-skill     Remove the Claude Code skill symlink
  resolve <ID...>     Resolve PR review threads by GraphQL ID
  watch <PR> [OPTS]   Monitor PR for new comments (--interval, --enrich, --notify)
  address <PR>        Interactive mode to work through issues one by one
  retrospective       Analyze patterns across all PRs (--since, --author, --enrich)

OPTIONS:
  --json            Output only JSON (default: combined)
  --markdown        Output only Markdown (default: combined)
  --output-dir DIR  Directory to save output (default: .reports/pr-reviews/pr-<N>)
  --enrich          Run Claude AI analysis on unresolved comment threads
  --diff            Include code diffs in Claude context (richer analysis)
  --prompt FILE     Custom prompt file for Claude analysis (overrides defaults)
  -h, --help        Show this help message
  -v, --version     Show version

EXAMPLES:
  gh pr-enrich install-skill              # Install Claude Code skill
  gh pr-enrich 123                        # Basic PR report
  gh pr-enrich 123 --enrich               # With Claude AI analysis
  gh pr-enrich 123 --enrich --diff        # Analysis with code context
  gh pr-enrich 123 --output-dir ./pr      # Custom output directory
  gh pr-enrich 123 --json                 # JSON output only
  gh pr-enrich resolve PRRT_xxx PRRT_yyy  # Resolve multiple threads
  gh pr-enrich watch 123 --enrich         # Monitor and auto-analyze
  gh pr-enrich address 123                # Interactive issue fixing

PROMPT CUSTOMIZATION:
  The Claude analysis prompt is loaded from (in priority order):
  1. --prompt FILE argument
  2. GH_PR_ENRICH_PROMPT environment variable
  3. .gh-pr-enrich-prompt.txt in current directory
  4. default-prompt.txt bundled with extension

  See default-prompt.txt in the extension directory for format.

ENVIRONMENT VARIABLES:
  PR_REVIEW_OUTPUT_ROOT   Override default output directory root
  GH_PR_ENRICH_PROMPT     Path to custom prompt file for Claude analysis

DEPENDENCIES:
  Required: gh (GitHub CLI), jq
  Optional: claude (Claude CLI, for --enrich)

For more info: https://github.com/bl4ck3lk/gh-pr-enrich
HELP
    exit 0
fi

PR_NUMBER=$1

# Validate PR_NUMBER is a positive integer (prevent path traversal)
if ! [[ "$PR_NUMBER" =~ ^[0-9]+$ ]]; then
    echo "Error: PR_NUMBER must be a positive integer, got: '$PR_NUMBER'"
    exit 1
fi

shift
OUTPUT_FORMAT="combined"
DEFAULT_OUTPUT_ROOT="${PR_REVIEW_OUTPUT_ROOT:-.reports/pr-reviews}"
OUTPUT_DIR="$DEFAULT_OUTPUT_ROOT/pr-$PR_NUMBER"
CUSTOM_OUTPUT_DIR=false
ENRICH_WITH_CLAUDE=false
INCLUDE_DIFF=false
CUSTOM_PROMPT_FILE=""
REPO=$(gh repo view --json nameWithOwner -q .nameWithOwner 2>/dev/null || echo "current-repo")
TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

# Get the directory where this extension is installed
get_extension_dir() {
    local script_path
    script_path="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    echo "$script_path"
}

# Load system prompt from file, filtering out comments
# Priority: --prompt arg > $GH_PR_ENRICH_PROMPT > .gh-pr-enrich-prompt.txt > default-prompt.txt
load_system_prompt() {
    local prompt_file=""
    local extension_dir
    extension_dir="$(get_extension_dir)"

    # Check --prompt argument first (highest priority)
    if [ -n "$CUSTOM_PROMPT_FILE" ] && [ -f "$CUSTOM_PROMPT_FILE" ]; then
        prompt_file="$CUSTOM_PROMPT_FILE"
    # Check environment variable
    elif [ -n "${GH_PR_ENRICH_PROMPT:-}" ] && [ -f "$GH_PR_ENRICH_PROMPT" ]; then
        prompt_file="$GH_PR_ENRICH_PROMPT"
    # Check for local repo override
    elif [ -f ".gh-pr-enrich-prompt.txt" ]; then
        prompt_file=".gh-pr-enrich-prompt.txt"
    # Fall back to bundled default
    elif [ -f "$extension_dir/default-prompt.txt" ]; then
        prompt_file="$extension_dir/default-prompt.txt"
    fi

    if [ -n "$prompt_file" ]; then
        # Filter out comment lines and leading blank lines (portable: works on BSD and GNU)
        grep -v '^#' "$prompt_file" | awk 'NF{found=1} found'
    else
        # Fallback if no file found (shouldn't happen normally)
        echo "You are a senior code reviewer analyzing unresolved PR comment threads.
Your task is to:
1. Categorize issues by type (security, performance, architecture, style, documentation, etc.)
2. Identify systemic patterns that appear across multiple comments
3. Suggest adjacent areas that may have similar problems worth investigating
4. Create a prioritized task list for addressing the issues

Focus on actionable insights. Be specific about file paths and patterns.
If there are no systemic issues or adjacent problems, return empty arrays for those fields."
    fi
}

# Parse options
while [[ $# -gt 0 ]]; do
    case $1 in
        --json)
            OUTPUT_FORMAT="json"
            shift
            ;;
        --markdown)
            OUTPUT_FORMAT="markdown"
            shift
            ;;
        --output-dir)
            if [ -z "${2:-}" ]; then
                echo "Error: --output-dir requires a directory path."
                exit 1
            fi
            CUSTOM_OUTPUT_DIR=true
            OUTPUT_DIR="$2"
            shift 2
            ;;
        --enrich)
            ENRICH_WITH_CLAUDE=true
            shift
            ;;
        --diff)
            INCLUDE_DIFF=true
            shift
            ;;
        --prompt)
            if [ -z "${2:-}" ]; then
                echo "Error: --prompt requires a file path."
                exit 1
            fi
            if [ ! -f "$2" ]; then
                echo "Error: Prompt file not found: $2"
                exit 1
            fi
            CUSTOM_PROMPT_FILE="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            echo "Run 'gh pr-enrich --help' for usage"
            exit 1
            ;;
    esac
done

# Ensure jq is available
if ! command -v jq &> /dev/null; then
    echo "Error: jq is required but not installed."
    echo "Install with: brew install jq (macOS) or apt-get install jq (Linux)"
    exit 1
fi

# Check Claude CLI if enrichment requested (warn but don't fail)
if [ "$ENRICH_WITH_CLAUDE" = true ]; then
    if ! command -v claude &> /dev/null; then
        echo "Warning: Claude CLI not found. Skipping enrichment."
        echo "Install from: https://claude.ai/code"
        ENRICH_WITH_CLAUDE=false
    fi
fi

# Create output directory
mkdir -p "$OUTPUT_DIR"

echo "Fetching details for PR #$PR_NUMBER in $REPO..."
echo "Saving report to: $OUTPUT_DIR"

# 1. Fetch PR summary
echo "Fetching PR summary..."
PR_SUMMARY_FIELDS="number,title,body,author,state,url,createdAt,updatedAt,mergedAt,closedAt,labels,assignees,reviewRequests,reviews,milestone,projectCards,additions,deletions,changedFiles,mergeable,isDraft,headRefName,headRefOid,baseRefName,baseRefOid,headRepository,headRepositoryOwner,files,commits,statusCheckRollup"
gh pr view "$PR_NUMBER" --json "$PR_SUMMARY_FIELDS" \
    > "$OUTPUT_DIR/pr-summary.json" 2>/dev/null || {
        echo "Error fetching PR summary. Ensure PR #$PR_NUMBER exists and you have access."
        exit 1
    }

# 2. Fetch all comments
echo "Fetching all comments..."
gh api --paginate "repos/$REPO/issues/$PR_NUMBER/comments" \
    | jq -s 'map(select(type == "array")) | (if length == 0 then [] else add end) | map(select(type == "object")) | map({
        id: .id,
        body: (.body // ""),
        user: .user.login,
        created_at: .created_at,
        updated_at: .updated_at,
        type: "issue_comment",
        html_url: .html_url
    })' > "$OUTPUT_DIR/issue-comments.json"

gh api --paginate "repos/$REPO/pulls/$PR_NUMBER/reviews" \
    | jq -s 'map(select(type == "array")) | (if length == 0 then [] else add end) | map(select(type == "object")) | map({
        id: .id,
        body: (.body // ""),
        user: .user.login,
        state: .state,
        submitted_at: .submitted_at,
        type: "review_comment",
        commit_id: .commit_id,
        html_url: .html_url
    })' > "$OUTPUT_DIR/review-comments.json"

gh api --paginate "repos/$REPO/pulls/$PR_NUMBER/comments" \
    | jq -s 'map(select(type == "array")) | (if length == 0 then [] else add end) | map(select(type == "object")) | map({
        id: .id,
        body: (.body // ""),
        path: .path,
        position: .position,
        line: .original_line,
        user: .user.login,
        created_at: .created_at,
        updated_at: .updated_at,
        type: "inline_comment",
        html_url: .html_url
    })' > "$OUTPUT_DIR/inline-comments.json"

# 3. Fetch comment threads with GraphQL IDs
echo "Fetching comment threads with GraphQL IDs..."
OWNER=$(echo "$REPO" | cut -d'/' -f1)
REPO_NAME=$(echo "$REPO" | cut -d'/' -f2)

GRAPHQL_QUERY='
query($owner: String!, $name: String!, $prNumber: Int!) {
  repository(owner: $owner, name: $name) {
    pullRequest(number: $prNumber) {
      reviewThreads(last: 100) {
        nodes {
          id
          isResolved
          comments(first: 10) {
            nodes {
              id
              databaseId
              body
              author {
                login
              }
              createdAt
              url
            }
          }
        }
      }
    }
  }
}
'

gh api graphql -F owner="$OWNER" -F name="$REPO_NAME" -F prNumber="$PR_NUMBER" -f query="$GRAPHQL_QUERY" \
    > "$OUTPUT_DIR/comment-threads.json" 2>/dev/null || {
        echo "Note: Could not fetch comment threads with GraphQL IDs."
        echo '{"data": {"repository": {"pullRequest": {"reviewThreads": {"nodes": []}}}}}' > "$OUTPUT_DIR/comment-threads.json"
    }

# Process comment threads to create ID mapping
jq -r '.data.repository.pullRequest.reviewThreads.nodes[] |
    select(.comments.nodes | length > 0) |
    .comments.nodes[] |
    select(.databaseId != null) |
    "\(.databaseId):\(.id)"' "$OUTPUT_DIR/comment-threads.json" > "$OUTPUT_DIR/id-mapping.txt" 2>/dev/null || {
        echo "" > "$OUTPUT_DIR/id-mapping.txt"
    }

# Function to add GraphQL IDs to comments
create_comments_with_graphql_ids() {
    local input_file="$1"
    local output_file="$2"
    local comment_type="$3"

    jq --rawfile id_map "$OUTPUT_DIR/id-mapping.txt" '
        map(. as $comment |
            . + {
                graphql_id: (
                    if ($comment.id and ($comment.id | type == "number")) then
                        ($id_map | split("\n") | map(select(length > 0) | split(":")) | map({key: .[0], value: .[1]}) | from_entries | .[$comment.id|tostring])
                    else
                        null
                    end
                ),
                comment_type: $comment_type
            }
        )
    ' "$input_file" --arg comment_type "$comment_type" > "$output_file"
}

# ============================================================================
# Claude Enrichment Functions
# ============================================================================

extract_unresolved_threads() {
    local threads_file="$1"
    local output_file="$2"

    jq '[.data.repository.pullRequest.reviewThreads.nodes[]
        | select(.isResolved == false)
        | {
            thread_id: .id,
            comments: [.comments.nodes[] | {
                author: (.author.login // "unknown"),
                body: .body,
                url: .url
            }]
        }]' "$threads_file" > "$output_file"
}

# Fetch the PR diff and create both raw and structured versions
# Args:
#   $1 - output_dir: Directory to save diff files
#   $2 - pr_num: PR number to fetch diff for
# Creates:
#   pr-diff.txt - Raw unified diff
#   pr-diff.json - Structured diff with file-level separation
fetch_pr_diff() {
    local output_dir="$1"
    local pr_num="$2"

    # Fetch the unified diff for the PR
    if ! gh pr diff "$pr_num" > "$output_dir/pr-diff.txt" 2>/dev/null; then
        echo "‚ö†Ô∏è  Could not fetch PR diff (binary files only, empty PR, or access denied)"
        echo "   Analysis will proceed without code context."
        echo "" > "$output_dir/pr-diff.txt"
    fi

    # Also create a structured version with file-level diffs
    # Truncate large diffs to avoid token limits in Claude API
    # 5000 chars per file balances context richness with API constraints
    jq -n --arg diff "$(<"$output_dir/pr-diff.txt")" \
        '{
            raw_diff: $diff,
            file_diffs: ($diff | split("diff --git ") | .[1:] | map(
                . as $chunk |
                ($chunk | split("\n") | .[0] | split(" ") | .[-1] | ltrimstr("b/")) as $filepath |
                {
                    file: $filepath,
                    content: ("diff --git " + $chunk)
                }
            ))
        }' > "$output_dir/pr-diff.json" 2>/dev/null || {
            echo '{"raw_diff": "", "file_diffs": []}' > "$output_dir/pr-diff.json"
        }
}

build_claude_context() {
    local output_dir="$1"
    local include_diff="${2:-false}"

    if [ "$include_diff" = "true" ] && [ -f "$output_dir/pr-diff.json" ]; then
        # Include diff in context for richer analysis
        jq -n \
            --argjson pr "$(<"$output_dir/pr-summary.json")" \
            --argjson unresolved "$(<"$output_dir/unresolved-threads.json")" \
            --argjson diff "$(<"$output_dir/pr-diff.json")" \
            '{
                pr: {
                    title: $pr.title,
                    body: ($pr.body // "No description"),
                    author: $pr.author.login,
                    files_changed: [$pr.files[].path]
                },
                unresolved_threads: $unresolved,
                code_changes: {
                    summary: "Diff included for context. Each file_diff contains the actual code changes.",
                    file_diffs: ($diff.file_diffs | map({
                        file: .file,
                        diff: (if (.content | length) > 5000 then
                            (.content | .[0:5000] + "\n... (truncated)")
                        else
                            .content
                        end)
                    }))
                }
            }' > "$output_dir/claude-context.json"
    else
        jq -n \
            --argjson pr "$(<"$output_dir/pr-summary.json")" \
            --argjson unresolved "$(<"$output_dir/unresolved-threads.json")" \
            '{
                pr: {
                    title: $pr.title,
                    body: ($pr.body // "No description"),
                    author: $pr.author.login,
                    files_changed: [$pr.files[].path]
                },
                unresolved_threads: $unresolved
            }' > "$output_dir/claude-context.json"
    fi
}

CLAUDE_ANALYSIS_SCHEMA='{
  "type": "object",
  "properties": {
    "issue_categories": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "name": { "type": "string" },
          "severity": { "enum": ["critical", "high", "medium", "low"] },
          "description": { "type": "string" },
          "thread_ids": { "type": "array", "items": { "type": "string" } }
        },
        "required": ["name", "severity", "description", "thread_ids"]
      }
    },
    "systemic_issues": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "pattern": { "type": "string" },
          "evidence": { "type": "array", "items": { "type": "string" } },
          "recommendation": { "type": "string" }
        },
        "required": ["pattern", "evidence", "recommendation"]
      }
    },
    "adjacent_problems": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "area": { "type": "string" },
          "risk": { "type": "string" },
          "investigation_hint": { "type": "string" }
        },
        "required": ["area", "risk", "investigation_hint"]
      }
    },
    "task_list": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "priority": { "enum": ["critical", "high", "medium", "low"] },
          "task": { "type": "string" },
          "thread_ids": { "type": "array", "items": { "type": "string" } }
        },
        "required": ["priority", "task", "thread_ids"]
      }
    },
    "process_improvements": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "category": { "enum": ["documentation", "automation", "testing", "review_process", "tooling"] },
          "suggestion": { "type": "string" },
          "rationale": { "type": "string" },
          "implementation_hint": { "type": "string" }
        },
        "required": ["category", "suggestion", "rationale"]
      }
    },
    "pr_template_suggestions": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "section": { "type": "string" },
          "checkbox_or_question": { "type": "string" },
          "why": { "type": "string" }
        },
        "required": ["section", "checkbox_or_question", "why"]
      }
    }
  },
  "required": ["issue_categories", "systemic_issues", "adjacent_problems", "task_list", "process_improvements", "pr_template_suggestions"]
}'

run_claude_analysis() {
    local context_file="$1"
    local output_file="$2"

    # Load system prompt from configurable file
    local system_prompt
    system_prompt="$(load_system_prompt)"

    local user_prompt="Analyze these unresolved PR comment threads and provide structured analysis:

$(cat "$context_file")"

    claude --print \
        --model sonnet \
        --output-format json \
        --json-schema "$CLAUDE_ANALYSIS_SCHEMA" \
        --system-prompt "$system_prompt" \
        "$user_prompt" > "$output_file" 2>/dev/null

    return $?
}

generate_analysis_report() {
    local analysis_file="$1"
    local output_file="$2"

    jq -r '
    "# ü§ñ Claude Analysis Report\n\n" +
    "## Issue Categories\n\n" +
    (if (.issue_categories | length) == 0 then
        "_No issue categories identified._\n\n"
    else
        (.issue_categories | map(
            "### " + .name + " (" + .severity + ")\n" +
            .description + "\n\n" +
            "**Threads:** " + (if (.thread_ids | length) > 0 then (.thread_ids | join(", ")) else "N/A" end) + "\n"
        ) | join("\n"))
    end) +
    "\n## Systemic Issues\n\n" +
    (if (.systemic_issues | length) == 0 then
        "_No systemic patterns identified._\n\n"
    else
        (.systemic_issues | map(
            "### " + .pattern + "\n\n" +
            "**Evidence:**\n" + (.evidence | map("- " + .) | join("\n")) + "\n\n" +
            "**Recommendation:** " + .recommendation + "\n"
        ) | join("\n"))
    end) +
    "\n## Adjacent Problems to Investigate\n\n" +
    (if (.adjacent_problems | length) == 0 then
        "_No adjacent problem areas identified._\n"
    else
        (.adjacent_problems | map(
            "- **" + .area + "** (" + .risk + "): " + .investigation_hint
        ) | join("\n"))
    end) +
    "\n\n## Task List\n\n" +
    (if (.task_list | length) == 0 then
        "_No tasks generated._\n"
    else
        (.task_list | map(
            "- [" + .priority + "] " + .task +
            (if (.thread_ids | length) > 0 then " _(threads: " + (.thread_ids | join(", ")) + ")_" else "" end)
        ) | join("\n"))
    end) +
    "\n\n## Process Improvements\n\n" +
    "_Suggestions to prevent similar issues in future PRs:_\n\n" +
    (if (.process_improvements | length) == 0 then
        "_No process improvements suggested._\n"
    else
        (.process_improvements | map(
            "### " + .suggestion + "\n" +
            "**Category:** " + .category + "\n\n" +
            .rationale + "\n" +
            (if .implementation_hint then "\n**How to implement:** " + .implementation_hint + "\n" else "" end)
        ) | join("\n"))
    end) +
    "\n\n## PR Template Suggestions\n\n" +
    "_Additions to the PR template that would catch these issues earlier:_\n\n" +
    (if (.pr_template_suggestions | length) == 0 then
        "_No PR template changes suggested._\n"
    else
        (.pr_template_suggestions | map(
            "### " + .section + "\n" +
            "```\n" + .checkbox_or_question + "\n```\n" +
            "_Why:_ " + .why + "\n"
        ) | join("\n"))
    end)
    ' "$analysis_file" > "$output_file"
}

# ============================================================================
# Main Processing
# ============================================================================

# Add GraphQL IDs to all comment types
create_comments_with_graphql_ids "$OUTPUT_DIR/issue-comments.json" "$OUTPUT_DIR/issue-comments-with-graphql.json" "issue_comment"
create_comments_with_graphql_ids "$OUTPUT_DIR/review-comments.json" "$OUTPUT_DIR/review-comments-with-graphql.json" "review_comment"
create_comments_with_graphql_ids "$OUTPUT_DIR/inline-comments.json" "$OUTPUT_DIR/inline-comments-with-graphql.json" "inline_comment"

# Combine comments
jq -s 'add | sort_by(.created_at // .submitted_at // "")' \
    "$OUTPUT_DIR"/issue-comments-with-graphql.json \
    "$OUTPUT_DIR"/review-comments-with-graphql.json \
    "$OUTPUT_DIR"/inline-comments-with-graphql.json \
    > "$OUTPUT_DIR/all-comments-with-graphql.json"

jq -s 'add | sort_by(.created_at // .submitted_at // "")' \
    "$OUTPUT_DIR"/issue-comments.json \
    "$OUTPUT_DIR"/review-comments.json \
    "$OUTPUT_DIR"/inline-comments.json \
    > "$OUTPUT_DIR/all-comments.json"

# 4. Fetch checks and status
echo "Fetching checks and status..."
gh pr checks "$PR_NUMBER" --json name,state,bucket,workflow,startedAt,completedAt,event,link,description > "$OUTPUT_DIR/checks.json" 2>/dev/null || {
    echo "Note: Could not fetch checks (may require additional permissions or no checks configured)."
    echo '[]' > "$OUTPUT_DIR/checks.json"
}

# 5. Generate statistics
echo "Generating statistics..."

COMMENT_STATS=$(jq -n \
    --argjson comments "$(cat "$OUTPUT_DIR/all-comments.json")" \
    '{
        total: ($comments | length),
        by_type: ($comments | group_by(.type) | map({type: .[0].type, count: length})),
        by_user: ($comments | group_by(.user) | map({user: .[0].user, count: length}) | sort_by(-.count) | .[0:10]),
        recent: ($comments | sort_by(.created_at) | reverse | .[0:5])
    }')

# Compute overall state from individual checks (bucket field: pass, fail, pending, skipping, cancel)
CHECKS_STATS=$(jq -r '{
    total: length,
    passing: (map(select(.bucket == "pass")) | length),
    failing: (map(select(.bucket == "fail")) | length),
    pending: (map(select(.bucket == "pending")) | length),
    skipped: (map(select(.bucket == "skipping")) | length),
    cancelled: (map(select(.bucket == "cancel")) | length),
    overall_state: (
        if length == 0 then "no_checks"
        elif (map(select(.bucket == "fail")) | length) > 0 then "failure"
        elif (map(select(.bucket == "pending")) | length) > 0 then "pending"
        elif (map(select(.bucket == "pass" or .bucket == "skipping" or .bucket == "cancel")) | length) == length then "success"
        else "mixed"
        end
    )
}' "$OUTPUT_DIR/checks.json" 2>/dev/null || echo '{"overall_state": "unknown", "total": 0, "passing": 0, "failing": 0, "pending": 0, "skipped": 0, "cancelled": 0}')

# 6. Create combined JSON
echo "Creating combined JSON..."
jq -n \
    --argjson summary "$(<"$OUTPUT_DIR/pr-summary.json")" \
    --argjson comments "$(<"$OUTPUT_DIR/all-comments-with-graphql.json")" \
    --argjson checks "$(<"$OUTPUT_DIR/checks.json")" \
    --argjson comment_stats "$COMMENT_STATS" \
    --argjson checks_stats "$CHECKS_STATS" \
    --argjson comment_threads "$(<"$OUTPUT_DIR/comment-threads.json")" \
    --arg timestamp "$TIMESTAMP" \
    --arg repo "$REPO" \
    '{
        metadata: {
            generated_at: $timestamp,
            repository: $repo,
            pr_number: ($summary.number | tonumber)
        },
        pr: $summary,
        comments: $comments,
        comment_threads: $comment_threads,
        checks: $checks,
        statistics: {
            comments: $comment_stats,
            checks: $checks_stats
        }
    }' > "$OUTPUT_DIR/combined-data.json"

# 7. Create Markdown report
echo "Creating Markdown report..."
cat > "$OUTPUT_DIR/comprehensive-report.md" << EOF
# Comprehensive PR Review Report: #$PR_NUMBER

**Repository:** $REPO
**Generated:** $TIMESTAMP
**PR URL:** $(jq -r '.url' "$OUTPUT_DIR/pr-summary.json")

## üìã PR Summary

**Title:** $(jq -r '.title' "$OUTPUT_DIR/pr-summary.json")
**Author:** $(jq -r '.author.login' "$OUTPUT_DIR/pr-summary.json")
**State:** $(jq -r '.state' "$OUTPUT_DIR/pr-summary.json")
**Created:** $(jq -r '.createdAt' "$OUTPUT_DIR/pr-summary.json")
**Updated:** $(jq -r '.updatedAt' "$OUTPUT_DIR/pr-summary.json")
**Mergeable:** $(jq -r '.mergeable' "$OUTPUT_DIR/pr-summary.json")
**Draft:** $(jq -r '.isDraft' "$OUTPUT_DIR/pr-summary.json")

**Changes:**
- Files: $(jq -r '.changedFiles' "$OUTPUT_DIR/pr-summary.json")
- Additions: $(jq -r '.additions' "$OUTPUT_DIR/pr-summary.json")
- Deletions: $(jq -r '.deletions' "$OUTPUT_DIR/pr-summary.json")

**Labels:** $(jq -r '.labels[]?.name // empty' "$OUTPUT_DIR/pr-summary.json" | tr '\n' ', ' | sed 's/,$//')
**Assignees:** $(jq -r '.assignees[]?.login // empty' "$OUTPUT_DIR/pr-summary.json" | tr '\n' ', ' | sed 's/,$//')
**Reviewers:** $(jq -r '.requestedReviewers[]?.login // empty' "$OUTPUT_DIR/pr-summary.json" | tr '\n' ', ' | sed 's/,$//')

### Files Changed
$(jq -r '.files[] | "- **\(.path)** (+\(.additions) / -\(.deletions))"' "$OUTPUT_DIR/pr-summary.json")

## ‚úÖ Checks & Status

**Overall State:** $(echo "$CHECKS_STATS" | jq -r '.overall_state // "unknown"')
**Total Checks:** $(echo "$CHECKS_STATS" | jq -r '.total // 0')
**Passing:** $(echo "$CHECKS_STATS" | jq -r '.passing // 0') | **Failing:** $(echo "$CHECKS_STATS" | jq -r '.failing // 0') | **Pending:** $(echo "$CHECKS_STATS" | jq -r '.pending // 0')

$(jq -r 'if length > 0 then "### Check Details\n" + (map("- **\(.name)**: \(.state) (\(.bucket))" + (if ((.workflow? | type) == "object" and (.workflow.name? != null)) then " - \(.workflow.name)" elif (.workflow? | type) == "string" then " - \(.workflow)" else "" end)) | join("\n")) else "No checks configured for this PR." end' "$OUTPUT_DIR/checks.json")

## üí¨ Comments Analysis

$(echo "$COMMENT_STATS" | jq -r '
"**Total Comments:** " + ((.total // 0) | tostring) + "\n\n" +
"### By Type\n" + ((.by_type // []) | map("- **" + (.type // "unknown") + ":** " + ((.count // 0) | tostring)) | join("\n")) + "\n\n" +
"### Top Commenters\n" + ((.by_user // []) | map("- **" + (.user // "unknown") + ":** " + ((.count // 0) | tostring)) | join("\n"))
')

## üßµ Comment Threads

To resolve a comment thread:
\`\`\`bash
gh api graphql -f query='mutation { resolveReviewThread(input: {threadId: "<THREAD_ID>"}) { thread { isResolved } } }'
\`\`\`

---

*Report generated by gh-pr-enrich v$VERSION at $TIMESTAMP*
EOF

# 8. Optional: Claude enrichment
if [ "$ENRICH_WITH_CLAUDE" = true ]; then
    echo "Running Claude analysis on unresolved threads..."

    extract_unresolved_threads "$OUTPUT_DIR/comment-threads.json" "$OUTPUT_DIR/unresolved-threads.json"

    UNRESOLVED_COUNT=$(jq 'length' "$OUTPUT_DIR/unresolved-threads.json")

    if [ "$UNRESOLVED_COUNT" -gt 0 ]; then
        echo "Found $UNRESOLVED_COUNT unresolved thread(s). Analyzing with Claude Sonnet..."

        # Fetch diff if requested
        if [ "$INCLUDE_DIFF" = true ]; then
            echo "Fetching PR diff for richer context..."
            fetch_pr_diff "$OUTPUT_DIR" "$PR_NUMBER"
        fi

        build_claude_context "$OUTPUT_DIR" "$INCLUDE_DIFF"

        if run_claude_analysis "$OUTPUT_DIR/claude-context.json" "$OUTPUT_DIR/claude-raw-response.json"; then
            echo "‚úÖ Claude analysis complete"

            if jq -e '.structured_output' "$OUTPUT_DIR/claude-raw-response.json" > /dev/null 2>&1; then
                jq '.structured_output' "$OUTPUT_DIR/claude-raw-response.json" > "$OUTPUT_DIR/claude-analysis.json"
            else
                cp "$OUTPUT_DIR/claude-raw-response.json" "$OUTPUT_DIR/claude-analysis.json"
            fi

            generate_analysis_report "$OUTPUT_DIR/claude-analysis.json" "$OUTPUT_DIR/claude-analysis.md"

            jq --argjson analysis "$(<"$OUTPUT_DIR/claude-analysis.json")" \
               '. + {claude_analysis: $analysis}' \
               "$OUTPUT_DIR/combined-data.json" > "$OUTPUT_DIR/combined-data.tmp.json" && \
               mv "$OUTPUT_DIR/combined-data.tmp.json" "$OUTPUT_DIR/combined-data.json"

            cat >> "$OUTPUT_DIR/comprehensive-report.md" << 'ANALYSIS_SECTION'

---

ANALYSIS_SECTION
            cat "$OUTPUT_DIR/claude-analysis.md" >> "$OUTPUT_DIR/comprehensive-report.md"
        else
            echo "‚ö†Ô∏è  Claude analysis failed. Continuing without enrichment."
        fi
    else
        echo "No unresolved threads found. Skipping Claude analysis."
    fi
fi

# Output
case $OUTPUT_FORMAT in
    "json")
        cat "$OUTPUT_DIR/combined-data.json"
        ;;
    "markdown")
        cat "$OUTPUT_DIR/comprehensive-report.md"
        ;;
    "combined")
        echo "‚úÖ Report generated successfully!"
        echo "üìÅ Directory: $OUTPUT_DIR"
        echo "üìä Markdown: $OUTPUT_DIR/comprehensive-report.md"
        echo "ü§ñ JSON: $OUTPUT_DIR/combined-data.json"
        if [ -f "$OUTPUT_DIR/claude-analysis.json" ]; then
            echo "üß† Claude Analysis: $OUTPUT_DIR/claude-analysis.md"
        fi
        ;;
esac
